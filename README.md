# Parallel_Lab
This repo is created to learn paralleism technology

## ğŸ“‹ CUDA

flash attention v1 & v2

 - [x] Update the flash attention v2 using CUDA basd on the [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)
 - [ ] Optimizing the flash attention v2 with MMA and other CUDA tricks
 - 
## ğŸ“– References ([Â©ï¸backğŸ‘†ğŸ»](#contents))
<div id="ref"></div>  

- [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)