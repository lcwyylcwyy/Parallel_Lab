# Parallel_Lab
This repo is created to learn paralleism technology

## 📋 CUDA

flash attention v1 & v2

 - [x] Update the flash attention v2 using CUDA basd on the [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)
 - [ ] Optimizing the flash attention v2 with MMA and other CUDA tricks
 - 
## 📖 References ([©️back👆🏻](#contents))
<div id="ref"></div>  

- [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)