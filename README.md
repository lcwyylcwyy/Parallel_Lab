# Parallel_Lab
This repo is created to learn paralleism technology

## ğŸ“‹ DNN and Attention

# flash attention v1 & v2

 - [x] Update the flash attention v2 using CUDA basd on the [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)
 - [ ] Optimizing the flash attention v2 with MMA and other CUDA tricks

# Optimizer

 - [x] SGD/SGDm
 - [x] AdaGrad/RMSprop/AdaDelta
 - [x] Adam/AdamW
 - [ ] Adafactor

# Attention
 - [x] Add Activation code for numpy and validate them with PyTorch\
 - 
## ğŸ“– References ([Â©ï¸backğŸ‘†ğŸ»](#contents))
<div id="ref"></div>  

- [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)