# Parallel_Lab
This repo is created to learn paralleism technology

## ğŸ“‹ CUDA

flash attention v1 & v2

I develop the flash attention v2 using CUDA basd on the [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)

## ğŸ“– References ([Â©ï¸backğŸ‘†ğŸ»](#contents))
<div id="ref"></div>  

- [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)