# Parallel_Lab
This repo is created to learn paralleism technology

## 📋 CUDA

flash attention v1 & v2

I develop the flash attention v2 using CUDA basd on the [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)

## 📖 References ([©️back👆🏻](#contents))
<div id="ref"></div>  

- [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)